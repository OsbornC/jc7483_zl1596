{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD \n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "meta = pd.read_csv(\"./AutoKaggle - Metadata.csv\", encoding='latin-1', error_bad_lines=False)\n",
    "arrOfRows = [65,360,239]\n",
    "nlp_rows = [239]\n",
    "tabular_rows = [65,360,241]\n",
    "\n",
    "row = 65\n",
    "train = ''\n",
    "data_name = meta['name'].loc[row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessing(row):\n",
    "    find_row = meta.loc[row]\n",
    "    train = ''\n",
    "    test = None\n",
    "    check_pred = True # if true, there exists a test dataset to submit\n",
    "    train_X = ''\n",
    "    train_Y = ''\n",
    "    pred_X = None\n",
    "    print(meta['name'].loc[row])\n",
    "    \n",
    "    if meta['name'].loc[row] == 'kobe-bryant-shot-selection':\n",
    "        train = pd.read_csv(\"./kobe-bryant-shot-selection/data/data.csv\")\n",
    "        check_pred = False\n",
    "    elif meta['name'].loc[row] == 'mercedes-benz-greener-manufacturing':\n",
    "        train = pd.read_csv(\"./mercedes-benz-greener-manufacturing/data/train.csv\")\n",
    "        pred = pd.read_csv(\"./mercedes-benz-greener-manufacturing/data/test.csv\")\n",
    "    elif meta['name'].loc[row] == 'uciml_sms-spam-collection-dataset':\n",
    "        train = pd.read_csv(\"./uciml_sms-spam-collection-dataset/data/spam.csv\",  encoding='cp1252', error_bad_lines=False)\n",
    "        check_pred = False\n",
    "    else:\n",
    "        train = pd.read_csv(\"./nsharanh-1b-visa/h1b_kaggle.csv\") \n",
    "        check_pred = False\n",
    "    \n",
    "    if check_pred:\n",
    "        pred = pred.dropna()\n",
    "    if row in nlp_rows:\n",
    "        row = pd.read_csv(\"./uciml_sms-spam-collection-dataset/submission/row.csv\")\n",
    "        sms = train\n",
    "        row_prepro = row['preprocessing function call'][0]\n",
    "        prepro_ls = eval(row_prepro)\n",
    "        sms = eval(prepro_ls[0])\n",
    "        train = eval(prepro_ls[1])\n",
    "        return train\n",
    "    else:\n",
    "        train = train.dropna()\n",
    "        \n",
    "        if type(meta[\"unwanted column\"].loc[row]) is str:  # check if there's unwanted column\n",
    "            column_list = eval(meta[\"unwanted column\"].loc[row])\n",
    "            train.drop(column_list,axis=1)\n",
    "        \n",
    "        if type(meta[\"numeric column\"].loc[row]) is str:\n",
    "            numeric=eval(meta[\"numeric column\"].loc[row])\n",
    "        \n",
    "        for c in train.columns:\n",
    "            if train[c].dtype == 'object':  #deal with text\n",
    "                lbl = LabelEncoder() \n",
    "                if check_pred:\n",
    "                    lbl.fit(list(train[c].values) + list(test[c].values)) \n",
    "                    train[c] = lbl.transform(list(train[c].values))\n",
    "                    test[c] = lbl.transform(list(test[c].values))\n",
    "                else:\n",
    "                    lbl.fit(list(train[c].values))\n",
    "                    train[c] = lbl.transform(list(train[c].values))\n",
    "        targetName = find_row['targetName']\n",
    "        train_Y = train[targetName]\n",
    "        train_X = train.drop(columns=targetName)\n",
    "        \n",
    "        if type(meta[\"preprocessing function call\"].loc[row]) is not str: #check if there is preprocessing functions\n",
    "            print('No preprocessing')\n",
    "        \n",
    "        else:\n",
    "            preprocessing_func = eval(meta[\"preprocessing function call\"].loc[row])\n",
    "            for call in preprocessing_func:\n",
    "                print(call)\n",
    "                exec(call)\n",
    "        \n",
    "        train_X,test_X,train_Y,test_Y = train_test_split(train_X, train_Y, test_size=0.2)\n",
    "\n",
    "        if check_pred:\n",
    "            pred_X = pred\n",
    "            return train_X, train_Y,test_X,test_Y ,pred_X\n",
    "        else:\n",
    "            return train_X, train_Y, test_X,test_Y,None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    return \" \".join(text)\n",
    "\n",
    "def feature_extraction(row, X_train,X_test,X_pred):\n",
    "    if row in nlp_rows:\n",
    "        rowcsv = pd.read_csv(\"./uciml_sms-spam-collection-dataset/submission/row.csv\")\n",
    "        row_extract = rowcsv['featureExtractor function call'].loc[0]\n",
    "        sms = X_train\n",
    "        extract = eval(row_extract)\n",
    "        sms['message'] = eval(extract[0])\n",
    "        sms['message'] = eval(extract[1])\n",
    "        text_feat = sms['message'].apply(str).copy()\n",
    "        text_feat = eval(extract[2])\n",
    "        vectorizer = eval(extract[3])\n",
    "        features = eval(extract[4])\n",
    "        features_train, features_test, labels_train, labels_test = train_test_split(features, sms['label'], test_size=0.3)\n",
    "        return features_train, features_test, labels_train, labels_test\n",
    "    else:\n",
    "        if type(meta[\"featureExtractor function call\"].loc[row]) is not str:\n",
    "            print('not func')\n",
    "            return X_train,X_test,X_pred\n",
    "        \n",
    "        extraction_function_calls = str(meta[\"featureExtractor function call\"].loc[row])\n",
    "        extraction_function_calls = extraction_function_calls.split(\",\")\n",
    "        extraction_funtion_param = eval(meta[\"featureExtractor function call\"].loc[row])\n",
    "        function_nums = len(extraction_function_calls)\n",
    "        for i in range(function_nums):\n",
    "            str1 = extraction_function_calls[i]\n",
    "            str2 = extraction_funtion_param[i]\n",
    "            l_str = str1.split(\"(\")\n",
    "            l_str.insert(1,\"(\"+str2)\n",
    "            str_call = ''\n",
    "            str_call = str_call.join(l_str)\n",
    "            str_call = 'extractor' + '=' + str_call\n",
    "            exec(str_call, globals(), globals())\n",
    "            extracted_train = extractor.fit_transform(X_train)\n",
    "            extracted_test = extractor.fit_transform(X_test)\n",
    "            n_comp = extracted_train.shape[1]\n",
    "            for j in range(0, n_comp):\n",
    "                X_train['extractor'+ str(i)+\"_\"+str(j)] = extracted_train[:, j]\n",
    "                X_test['extractor'+ str(i)+\"_\"+str(j)] = extracted_test[:, j]\n",
    "            if X_pred is not None:\n",
    "                extracted_pred = extractor.fit_transform(X_pred)\n",
    "                for j in range(0, n_comp):\n",
    "                    X_test['extractor'+ str(i)+\"_\"+str(j)] = extracted_pred[:, j]\n",
    "                return X_train,X_test,X_pred\n",
    "            else:\n",
    "                return X_train,X_test,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimation(row,X_train,X_test,Y_train, Y_test):\n",
    "    if row in nlp_rows:\n",
    "        rowcsv = pd.read_csv(\"./uciml_sms-spam-collection-dataset/submission/row.csv\")\n",
    "        row_extract = eval(rowcsv['estimator1 function call'].loc[0])\n",
    "        mnb = eval(row_extract[0])\n",
    "        eval(row_extract[1])\n",
    "        pred = eval(row_extract[2])\n",
    "        if rowcsv['performanceMetric'].loc[0] == 'accuracy':\n",
    "            return accuracy_score(Y_test, pred)\n",
    "    else:\n",
    "        estimation_function_calls = eval(meta[\"estimator1 function call\"].loc[row])\n",
    "\n",
    "        \n",
    "        print(len(estimation_function_calls))\n",
    "        if len(estimation_function_calls) == 1:\n",
    "            str_call = estimation_function_calls[0]\n",
    "            print(str_call)\n",
    "            str_call = 'estimator' + '=' + str_call\n",
    "            print(str_call)\n",
    "            exec(str_call,globals(),globals())\n",
    "            \n",
    "            if meta[\"taskType\"].loc[row] == 'classification':\n",
    "                estimator.fit(X_train,Y_train)\n",
    "                Y_pred = estimator.predict(X_test)\n",
    "#                 print('here')\n",
    "                print(\"Accuracy:\", recall_score(Y_test,Y_pred,average='weighted'))\n",
    "#                 print('here')\n",
    "            elif meta[\"taskType\"].loc[row] == 'regression':\n",
    "                estimator.fit(X_train,Y_train)\n",
    "                print(\"Accuracy:\", r2_score(Y_test,Y_pred,average='weighted'))\n",
    "        else:\n",
    "            estimators = []\n",
    "            n_estimators = len(estimation_function_calls)\n",
    "            for i in range(n_estimators):\n",
    "                str1 = extraction_function_calls\n",
    "                str2 = extraction_funtion_param\n",
    "                l_str = str1.split(\"(\")\n",
    "                l_str.insert(1,\"(\"+str2)\n",
    "                str_call = ''\n",
    "                str_call = str_call.join(l_str)\n",
    "                str_call = 'estimator' + '=' + str_call\n",
    "#                 print(l_str)\n",
    "#                 print(str_call)\n",
    "                exec(str_call)\n",
    "                estimators.append(estimator)\n",
    "                postprocessing(estimators,stack = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kobe-bryant-shot-selection\n",
      "No preprocessing\n",
      "not func\n",
      "1\n",
      "XGBClassifier(max_depth=7, learning_rate=0.012, n_estimators=1000, subsample=0.62, colsample_bytree=0.6, seed=1)\n",
      "estimator=XGBClassifier(max_depth=7, learning_rate=0.012, n_estimators=1000, subsample=0.62, colsample_bytree=0.6, seed=1)\n",
      "Accuracy: 0.6799610894941635\n"
     ]
    }
   ],
   "source": [
    "if row in nlp_rows:\n",
    "    train_set = preprocessing(row)\n",
    "    X_train, X_test, Y_train, Y_test = feature_extraction(row, train_set, None, None)\n",
    "    Y_pred = estimation(row, X_train, X_test, Y_train, Y_test)\n",
    "    print(\"The accuracy of SMS Spam Collection Dataset is\", Y_pred)\n",
    "if row in tabular_rows:\n",
    "    X_train,Y_train,X_test,Y_test,X_pred = preprocessing(row)\n",
    "    X_train,X_test,X_pred = feature_extraction(row,X_train,X_test,X_pred)\n",
    "    estimation(row,X_train,X_test,Y_train, Y_test)\n",
    "    \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocessing(estimators,stack):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.140522956848145\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
