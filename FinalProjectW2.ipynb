{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD \n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xd5 in position 61: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xd5 in position 61: invalid continuation byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-d97ac79d79af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./AutoKaggle - Metadata.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0marrOfRows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m360\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m239\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m317\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mnlp_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m239\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xd5 in position 61: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "meta = pd.read_csv(\"./AutoKaggle - Metadata.csv\")\n",
    "arrOfRows = [64,360,239,317]\n",
    "nlp_rows = [239]\n",
    "row = 317\n",
    "train = ''\n",
    "def preprocessing(row):\n",
    "    print(meta['name'].loc[row])\n",
    "    find_row = meta.loc[row]\n",
    "    train = ''\n",
    "    test = None\n",
    "    check_test = True\n",
    "    train_X = ''\n",
    "    train_Y = ''\n",
    "    test_X = None\n",
    "    if meta['name'].loc[row] == 'kobe-bryant-shot-selection':\n",
    "        train = pd.read_csv(\"./kobe-bryant-shot-selection/data/data.csv\")\n",
    "        check_test = False\n",
    "    elif meta['name'].loc[row] == 'mercedes-benz-greener-manufacturing':\n",
    "        train = pd.read_csv(\"./mercedes-benz-greener-manufacturing/data/train.csv\")\n",
    "        test = pd.read_csv(\"./mercedes-benz-greener-manufacturing/data/test.csv\")\n",
    "    elif meta['name'].loc[row] == 'uciml_sms-spam-collection-dataset':\n",
    "        train = pd.read_csv(\"./uciml_sms-spam-collection-dataset/data/spam.csv\", encoding='cp1252', error_bad_lines=False)\n",
    "        check_test = False\n",
    "        \n",
    "    \n",
    "    if check_test:\n",
    "        test = test.dropna()\n",
    "    if meta['name'].loc[row] == 'uciml_sms-spam-collection-dataset':\n",
    "        row = pd.read_csv(\"./uciml_sms-spam-collection-dataset/submission/row.csv\", encoding='cp1252')\n",
    "        sms = train\n",
    "        row_prepro = row['preprocessing function call'][0]\n",
    "        prepro_ls = eval(row_prepro)\n",
    "        sms = eval(prepro_ls[0])\n",
    "        train = eval(prepro_ls[1])\n",
    "        return train\n",
    "    else:\n",
    "        train = train.dropna()\n",
    "        for c in train.columns:\n",
    "            if train[c].dtype == 'object':    # deal with text\n",
    "                lbl = LabelEncoder() \n",
    "                if check_test:\n",
    "                    lbl.fit(list(train[c].values) + list(test[c].values)) \n",
    "                    train[c] = lbl.transform(list(train[c].values))\n",
    "                    test[c] = lbl.transform(list(test[c].values))\n",
    "                else:\n",
    "                    lbl.fit(list(train[c].values))\n",
    "                    train[c] = lbl.transform(list(train[c].values))\n",
    "\n",
    "        targetName = find_row['targetName']\n",
    "        train_Y = train[targetName]\n",
    "        train_X = train.drop(columns=targetName)\n",
    "        if check_test:\n",
    "            test_X = test\n",
    "            return train_X, train_Y, test_X\n",
    "        else:\n",
    "            return train_X, train_Y, None\n",
    "\n",
    "\n",
    "preprocessing(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    return \" \".join(text)\n",
    "\n",
    "def feature_extraction(row, X_train, X_test):\n",
    "    if meta['name'].loc[row] == 'uciml_sms-spam-collection-dataset':\n",
    "        rowcsv = pd.read_csv(\"./uciml_sms-spam-collection-dataset/submission/row.csv\")\n",
    "        row_extract = rowcsv['featureExtractor function call'].loc[0]\n",
    "        sms = X_train\n",
    "        extract = eval(row_extract)\n",
    "        sms['message'] = eval(extract[0])\n",
    "        sms['message'] = eval(extract[1])\n",
    "        text_feat = sms['message'].apply(str).copy()\n",
    "        text_feat = eval(extract[2])\n",
    "        vectorizer = eval(extract[3])\n",
    "        features = eval(extract[4])\n",
    "        features_train, features_test, labels_train, labels_test = train_test_split(features, sms['label'], test_size=0.3)\n",
    "        return features_train, features_test, labels_train, labels_test\n",
    "    else:\n",
    "        if type(meta[\"featureExtractor function call\"].loc[row]) is not str:\n",
    "            print('not func')\n",
    "            return X_train,X_test\n",
    "        extraction_function_calls = str(meta[\"function call feature extraction\"].loc[row])\n",
    "        extraction_function_calls = extraction_function_calls.split(\",\")\n",
    "        extraction_funtion_param = eval(meta[\"function parameters feature extraction\"].loc[row])\n",
    "        function_nums = len(extraction_function_calls)\n",
    "        for i in range(function_nums):\n",
    "            str1 = extraction_function_calls[i]\n",
    "            str2 = extraction_funtion_param[i]\n",
    "            l_str = str1.split(\"(\")\n",
    "            l_str.insert(1,\"(\"+str2)\n",
    "            str_call = ''\n",
    "            str_call = str_call.join(l_str)\n",
    "            str_call = 'extractor' + '=' + str_call\n",
    "            exec(str_call, globals(), globals())\n",
    "            extracted_train = extractor.fit_transform(X_train)\n",
    "            n_comp = extracted_train.shape[1]\n",
    "            for j in range(0, n_comp):\n",
    "                X_train['extractor'+ str(i)+\"_\"+str(j)] = extracted_train[:, j]\n",
    "            if X_test is not None:\n",
    "                extracted_test = extractor.fit_transform(X_test)\n",
    "                for j in range(0, n_comp):\n",
    "                    X_test['extractor'+ str(i)+\"_\"+str(j)] = extracted_test[:, j]\n",
    "                return X_train,X_test\n",
    "            else:\n",
    "                return X_train,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'row' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-c653129128f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'row' is not defined"
     ]
    }
   ],
   "source": [
    "train = preprocessing(row)\n",
    "feature_extraction(row, train, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimation(row,X_train,X_test,Y_train, Y_test):\n",
    "    if row in nlp_rows:\n",
    "        rowcsv = pd.read_csv(\"./uciml_sms-spam-collection-dataset/submission/row.csv\")\n",
    "        row_extract = eval(rowcsv['estimator1 function call'].loc[0])\n",
    "        mnb = eval(row_extract[0])\n",
    "        eval(row_extract[1])\n",
    "        pred = eval(row_extract[2])\n",
    "        if rowcsv['performanceMetric'].loc[0] == 'accuracy':\n",
    "            return accuracy_score(Y_test, pred)\n",
    "    else:\n",
    "        estimation_function_calls = eval(meta[\"estimator1 function call\"].loc[row])\n",
    "        print(len(estimation_function_calls))\n",
    "        if len(estimation_function_calls) == 1:\n",
    "            str_call = estimation_function_calls[0]\n",
    "            print(str_call)\n",
    "            str_call = 'estimator' + '=' + str_call\n",
    "            print(str_call)\n",
    "            exec(str_call,globals(),globals())\n",
    "            \n",
    "            if meta[\"taskType\"].loc[row] == 'classification':\n",
    "                estimator.fit(X_train,Y_train)\n",
    "                Y_pred = estimator.predict(X_test)\n",
    "                print('here')\n",
    "                print(recall_score(Y_test,Y_pred,average='weighted'))\n",
    "                print('here')\n",
    "            elif meta[\"taskType\"].loc[row] == 'regression':\n",
    "                estimator.fit(X_train,Y_train)\n",
    "                print(r2_score(Y_test,Y_pred,average='weighted'))\n",
    "        else:\n",
    "            estimators = []\n",
    "            n_estimators = len(estimation_function_calls)\n",
    "            for i in range(n_estimators):\n",
    "                str1 = extraction_function_calls\n",
    "                str2 = extraction_funtion_param\n",
    "                l_str = str1.split(\"(\")\n",
    "                l_str.insert(1,\"(\"+str2)\n",
    "                str_call = ''\n",
    "                str_call = str_call.join(l_str)\n",
    "                str_call = 'estimator' + '=' + str_call\n",
    "                print(l_str)\n",
    "                print(str_call)\n",
    "                exec(str_call)\n",
    "                estimators.append(estimator)\n",
    "                postprocessing(estimators,stack = True)\n",
    "#     if row in nlp_rows:\n",
    "#         rowcsv = pd.read_csv(\"./uciml_sms-spam-collection-dataset/submission/row.csv\")\n",
    "#         row_extract = eval(rowcsv['estimator1 function call'].loc[0])\n",
    "#         mnb = eval(row_extract[0])\n",
    "#         eval(row_extract[1])\n",
    "#         pred = eval(row_extract[2])\n",
    "#         if rowcsv['performanceMetric'].loc[0] == 'accuracy':\n",
    "#             return accuracy_score(Y_test, pred)\n",
    "#     else:\n",
    "#         estimation_function_calls = meta[\"function calls estimation\"].loc[row]\n",
    "#         estimation_function_calls = estimation_function_calls.split(\",\")\n",
    "#         print(type(meta[\"function parameters estimation\"].loc[row]))\n",
    "#         print(meta[\"function parameters estimation\"].loc[row])\n",
    "#         estimation_function_param = eval(meta[\"function parameters estimation\"].loc[row])\n",
    "\n",
    "#         print(len(estimation_function_calls))\n",
    "#         if len(estimation_function_calls) == 1:\n",
    "#             l_str = estimation_function_calls[0].split(\"(\")\n",
    "#             l_str.insert(1,'('+estimation_function_param[0])\n",
    "#             str_call = ''\n",
    "#             str_call = str_call.join(l_str)\n",
    "#             str_call = 'estimator' + '=' + str_call\n",
    "#             exec(str_call,globals(),globals())\n",
    "#     #         estimator.fit(X_train,Y_train)\n",
    "#             print(cross_val_score(estimator, X_train, Y_train, cv=3, n_jobs=8))\n",
    "#         else:\n",
    "#             estimators = []\n",
    "#             n_estimators = len(estimation_function_calls)\n",
    "#             for i in range(n_estimators):\n",
    "#                 str1 = extraction_function_calls\n",
    "#                 str2 = extraction_funtion_param\n",
    "#                 l_str = str1.split(\"(\")\n",
    "#                 l_str.insert(1,\"(\"+str2)\n",
    "#                 str_call = ''\n",
    "#                 str_call = str_call.join(l_str)\n",
    "#                 str_call = 'estimator' + '=' + str_call\n",
    "#                 print(l_str)\n",
    "#                 print(str_call)\n",
    "#                 exec(str_call)\n",
    "#                 estimators.append(estimator)\n",
    "#                 postprocessing(estimators,stack = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'row' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-da85018ce759>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mY_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'shot_made_flag'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'row' is not defined"
     ]
    }
   ],
   "source": [
    "train_set = preprocessing(row)\n",
    "X_train, X_test, Y_train, Y_test = feature_extraction(row, train_set, None)\n",
    "Y_pred = estimation(row, ['shot_made_flag'], None, None, None)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-7ea92b9aa1d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mmeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'uciml_sms-spam-collection-dataset'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mtrain_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mY_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'meta' is not defined"
     ]
    }
   ],
   "source": [
    "if meta['name'].loc[row] == 'uciml_sms-spam-collection-dataset':\n",
    "    train_set = preprocessing(row)\n",
    "    X_train, X_test, Y_train, Y_test = feature_extraction(row, train_set, None)\n",
    "    Y_pred = estimation(row, X_train, X_test, Y_train, Y_test)\n",
    "    print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocessing(estimators,stack):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use 2.0979933738708496 seconds to run this.\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Use\", end - start, \"seconds to run this.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
