{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD \n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.signal import decimate\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPool1D, GlobalAvgPool1D, Dropout, BatchNormalization, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "meta = pd.read_csv(\"./AutoKaggle - Metadata.csv\", encoding='cp1252')\n",
    "arrOfRows = [64,360,239,198]\n",
    "nlp_rows = [239]\n",
    "cnn_rows = [196]\n",
    "row = 196\n",
    "train = ''\n",
    "\n",
    "INPUT_LIB = './kinguistics_heartbeat-sounds/data/'\n",
    "SAMPLE_RATE = 44100\n",
    "CLASSES = ['artifact', 'normal', 'murmur']\n",
    "CODE_BOOK = {x:i for i,x in enumerate(CLASSES)}   \n",
    "NB_CLASSES = len(CLASSES)\n",
    "def preprocessing(row):\n",
    "    find_row = meta.loc[row]\n",
    "    train = ''\n",
    "    test = None\n",
    "    check_test = True\n",
    "    train_X = ''\n",
    "    train_Y = ''\n",
    "    test_X = None\n",
    "    if meta['name'].loc[row] == 'kobe-bryant-shot-selection':\n",
    "        train = pd.read_csv(\"./kobe-bryant-shot-selection/data/data.csv\", encoding='cp1252')\n",
    "        check_test = False\n",
    "    elif meta['name'].loc[row] == 'mercedes-benz-greener-manufacturing':\n",
    "        train = pd.read_csv(\"./mercedes-benz-greener-manufacturing/data/train.csv\")\n",
    "        test = pd.read_csv(\"./mercedes-benz-greener-manufacturing/data/test.csv\")\n",
    "    elif meta['name'].loc[row] == 'uciml_sms-spam-collection-dataset':\n",
    "        train = pd.read_csv(\"./uciml_sms-spam-collection-dataset/data/spam.csv\", encoding='cp1252', error_bad_lines=False)\n",
    "        check_test = False\n",
    "    elif meta['name'].loc[row] == 'kinguistics_heartbeat-sounds':\n",
    "        train = []\n",
    "        train.append(pd.read_csv(\"./kinguistics_heartbeat-sounds/data/set_a_timing.csv\"))\n",
    "        train.append(pd.read_csv(\"./kinguistics_heartbeat-sounds/data/set_a.csv\"))\n",
    "        train.append(pd.read_csv(\"./kinguistics_heartbeat-sounds/data/set_b.csv\"))\n",
    "        check_test = False\n",
    "        \n",
    "    \n",
    "    if check_test:\n",
    "        test = test.dropna()\n",
    "    if meta['name'].loc[row] == 'uciml_sms-spam-collection-dataset':\n",
    "        row = pd.read_csv(\"./uciml_sms-spam-collection-dataset/submission/row.csv\", encoding='cp1252')\n",
    "        sms = train\n",
    "        row_prepro = row['preprocessing function call'][0]\n",
    "        prepro_ls = eval(row_prepro)\n",
    "        sms = eval(prepro_ls[0])\n",
    "        train = eval(prepro_ls[1])\n",
    "        return train\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    elif meta['name'].loc[row] == 'kinguistics_heartbeat-sounds':\n",
    "        row = pd.read_csv(\"./kinguistics_heartbeat-sounds/submission/row.csv\", encoding='cp1252')\n",
    "        prepro = eval(row['preprocessing function call'][0])\n",
    "        eval(prepro[0])\n",
    "        apply = eval(row['apply'][0])\n",
    "        apply_column = eval(row['apply_column'][0])\n",
    "        apply = eval(row['apply'][0])\n",
    "        apply_target = eval(row['apply_target'][0])\n",
    "        for i in range(len(apply)):\n",
    "            eval_str = \"train[1][\" + apply_target[i] + \"] = train[1][\" + apply_column[i] + \"].apply(\" + apply[i] + \")\"\n",
    "            exec(eval_str)\n",
    "            \n",
    "        x_data = np.stack(train[1]['time_series'].values, axis=0)\n",
    "        new_labels =[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1,\n",
    "             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, \n",
    "             2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, \n",
    "             2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, \n",
    "             1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 0, 2, 2, 1, 1, 1, 1, 1, \n",
    "             0, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, \n",
    "             1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
    "        new_labels = np.array(new_labels, dtype='int')\n",
    "        y_data = np_utils.to_categorical(new_labels)\n",
    "        x_train, x_test, y_train, y_test, train_filenames, test_filenames = \\\n",
    "        train_test_split(x_data, y_data, train[1]['fname'].values, test_size=0.25)\n",
    "        x_train = decimate(x_train, 8, axis=1, zero_phase=True)\n",
    "        x_train = decimate(x_train, 8, axis=1, zero_phase=True)\n",
    "        x_train = decimate(x_train, 4, axis=1, zero_phase=True)\n",
    "        x_test = decimate(x_test, 8, axis=1, zero_phase=True)\n",
    "        x_test = decimate(x_test, 8, axis=1, zero_phase=True)\n",
    "        x_test = decimate(x_test, 4, axis=1, zero_phase=True)\n",
    "        x_train = x_train / np.std(x_train, axis=1).reshape(-1,1)\n",
    "        x_test = x_test / np.std(x_test, axis=1).reshape(-1,1)\n",
    "        x_train = x_train[:,:,np.newaxis]\n",
    "        x_test = x_test[:,:,np.newaxis]\n",
    "        return x_train, x_test, y_train, y_test\n",
    "    else:\n",
    "        train = train.dropna()\n",
    "        for c in train.columns:\n",
    "            if train[c].dtype == 'object':    # deal with text\n",
    "                lbl = LabelEncoder() \n",
    "                if check_test:\n",
    "                    lbl.fit(list(train[c].values) + list(test[c].values)) \n",
    "                    train[c] = lbl.transform(list(train[c].values))\n",
    "                    test[c] = lbl.transform(list(test[c].values))\n",
    "                else:\n",
    "                    lbl.fit(list(train[c].values))\n",
    "                    train[c] = lbl.transform(list(train[c].values))\n",
    "\n",
    "        targetName = find_row['targetName']\n",
    "        train_Y = train[targetName]\n",
    "        train_X = train.drop(columns=targetName)\n",
    "        if check_test:\n",
    "            test_X = test\n",
    "            return train_X, train_Y, test_X\n",
    "        else:\n",
    "            return train_X, train_Y, None\n",
    "\n",
    "        \n",
    "def clean_filename(fname, string):   \n",
    "    file_name = fname.split('/')[1]\n",
    "    if file_name[:2] == '__':  \n",
    "        file_name = string + file_name\n",
    "    return file_name\n",
    "\n",
    "def load_wav_file(name, path):\n",
    "    _, b = wavfile.read(path + name)\n",
    "    assert _ == SAMPLE_RATE\n",
    "    return b\n",
    "\n",
    "def repeat_to_length(arr, length):\n",
    "    \"\"\"Repeats the numpy 1D array to given length, and makes datatype float\"\"\"\n",
    "    result = np.empty((length, ), dtype = 'float32')\n",
    "    l = len(arr)\n",
    "    pos = 0\n",
    "    while pos + l <= length:\n",
    "        result[pos:pos+l] = arr\n",
    "        pos += l\n",
    "    if pos < length:\n",
    "        result[pos:length] = arr[:length-pos]\n",
    "    return result\n",
    "\n",
    "\n",
    "print(meta['name'].loc[row])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    return \" \".join(text)\n",
    "\n",
    "def feature_extraction(row, X_train, X_test):\n",
    "    if meta['name'].loc[row] == 'uciml_sms-spam-collection-dataset':\n",
    "        rowcsv = pd.read_csv(\"./uciml_sms-spam-collection-dataset/submission/row.csv\")\n",
    "        row_extract = rowcsv['featureExtractor function call'].loc[0]\n",
    "        sms = X_train\n",
    "        extract = eval(row_extract)\n",
    "        sms['message'] = eval(extract[0])\n",
    "        sms['message'] = eval(extract[1])\n",
    "        text_feat = sms['message'].apply(str).copy()\n",
    "        text_feat = eval(extract[2])\n",
    "        vectorizer = eval(extract[3])\n",
    "        features = eval(extract[4])\n",
    "        features_train, features_test, labels_train, labels_test = train_test_split(features, sms['label'], test_size=0.3)\n",
    "        return features_train, features_test, labels_train, labels_test\n",
    "    else:\n",
    "        if type(meta[\"featureExtractor function call\"].loc[row]) is not str:\n",
    "            print('not func')\n",
    "            return X_train,X_test\n",
    "        extraction_function_calls = str(meta[\"function call feature extraction\"].loc[row])\n",
    "        extraction_function_calls = extraction_function_calls.split(\",\")\n",
    "        extraction_funtion_param = eval(meta[\"function parameters feature extraction\"].loc[row])\n",
    "        function_nums = len(extraction_function_calls)\n",
    "        for i in range(function_nums):\n",
    "            str1 = extraction_function_calls[i]\n",
    "            str2 = extraction_funtion_param[i]\n",
    "            l_str = str1.split(\"(\")\n",
    "            l_str.insert(1,\"(\"+str2)\n",
    "            str_call = ''\n",
    "            str_call = str_call.join(l_str)\n",
    "            str_call = 'extractor' + '=' + str_call\n",
    "            exec(str_call, globals(), globals())\n",
    "            extracted_train = extractor.fit_transform(X_train)\n",
    "            n_comp = extracted_train.shape[1]\n",
    "            for j in range(0, n_comp):\n",
    "                X_train['extractor'+ str(i)+\"_\"+str(j)] = extracted_train[:, j]\n",
    "            if X_test is not None:\n",
    "                extracted_test = extractor.fit_transform(X_test)\n",
    "                for j in range(0, n_comp):\n",
    "                    X_test['extractor'+ str(i)+\"_\"+str(j)] = extracted_test[:, j]\n",
    "                return X_train,X_test\n",
    "            else:\n",
    "                return X_train,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'dropna'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c653129128f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-7d2521c17631>\u001b[0m in \u001b[0;36mpreprocessing\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcheck_test\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'uciml_sms-spam-collection-dataset'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./uciml_sms-spam-collection-dataset/submission/row.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cp1252'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'dropna'"
     ]
    }
   ],
   "source": [
    "train = preprocessing(row)\n",
    "feature_extraction(row, train, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(x_train, y_train, batch_size):\n",
    "    \"\"\"\n",
    "    Rotates the time series randomly in time\n",
    "    \"\"\"\n",
    "    x_batch = np.empty((batch_size, x_train.shape[1], x_train.shape[2]), dtype='float32')\n",
    "    y_batch = np.empty((batch_size, y_train.shape[1]), dtype='float32')\n",
    "    full_idx = range(x_train.shape[0])\n",
    "    \n",
    "    while True:\n",
    "        batch_idx = np.random.choice(full_idx, batch_size)\n",
    "        x_batch = x_train[batch_idx]\n",
    "        y_batch = y_train[batch_idx]\n",
    "    \n",
    "        for i in range(batch_size):\n",
    "            sz = np.random.randint(x_batch.shape[1])\n",
    "            x_batch[i] = np.roll(x_batch[i], sz, axis = 0)\n",
    "     \n",
    "        yield x_batch, y_batch\n",
    "def estimation(row,X_train,X_test,Y_train, Y_test):\n",
    "    if row in nlp_rows:\n",
    "        rowcsv = pd.read_csv(\"./uciml_sms-spam-collection-dataset/submission/row.csv\")\n",
    "        row_extract = eval(rowcsv['estimator1 function call'].loc[0])\n",
    "        mnb = eval(row_extract[0])\n",
    "        eval(row_extract[1])\n",
    "        pred = eval(row_extract[2])\n",
    "        if rowcsv['performanceMetric'].loc[0] == 'accuracy':\n",
    "            return accuracy_score(Y_test, pred)\n",
    "    elif row in cnn_rows:\n",
    "        rowcsv = pd.read_csv(\"./kinguistics_heartbeat-sounds/submission/row.csv\")\n",
    "        cnn_callback = eval(rowcsv['cnn_callback'].loc[0])\n",
    "        callback = [0, 0]\n",
    "        for i in range(len(cnn_callback)):\n",
    "            exec(cnn_callback[i])\n",
    "        estimator1_function_call = eval(rowcsv['estimator1 function call'].loc[0])\n",
    "        model = eval(estimator1_function_call[0])\n",
    "        cnn_train = eval(rowcsv['cnn_train'].loc[0])\n",
    "        for i in range(len(cnn_train)):\n",
    "            exec(\"model.\" + cnn_train[i])\n",
    "        scores = model.evaluate(X_test, Y_test, verbose=1)\n",
    "        print('Test loss:', scores[0])\n",
    "        print('Test accuracy:', scores[1])    \n",
    "    else:\n",
    "        estimation_function_calls = eval(meta[\"estimator1 function call\"].loc[row])\n",
    "        print(len(estimation_function_calls))\n",
    "        if len(estimation_function_calls) == 1:\n",
    "            str_call = estimation_function_calls[0]\n",
    "            print(str_call)\n",
    "            str_call = 'estimator' + '=' + str_call\n",
    "            print(str_call)\n",
    "            exec(str_call,globals(),globals())\n",
    "            \n",
    "            if meta[\"taskType\"].loc[row] == 'classification':\n",
    "                estimator.fit(X_train,Y_train)\n",
    "                Y_pred = estimator.predict(X_test)\n",
    "                print('here')\n",
    "                print(recall_score(Y_test,Y_pred,average='weighted'))\n",
    "                print('here')\n",
    "            elif meta[\"taskType\"].loc[row] == 'regression':\n",
    "                estimator.fit(X_train,Y_train)\n",
    "                print(r2_score(Y_test,Y_pred,average='weighted'))\n",
    "        else:\n",
    "            estimators = []\n",
    "            n_estimators = len(estimation_function_calls)\n",
    "            for i in range(n_estimators):\n",
    "                str1 = extraction_function_calls\n",
    "                str2 = extraction_funtion_param\n",
    "                l_str = str1.split(\"(\")\n",
    "                l_str.insert(1,\"(\"+str2)\n",
    "                str_call = ''\n",
    "                str_call = str_call.join(l_str)\n",
    "                str_call = 'estimator' + '=' + str_call\n",
    "                print(l_str)\n",
    "                print(str_call)\n",
    "                exec(str_call)\n",
    "                estimators.append(estimator)\n",
    "                postprocessing(estimators,stack = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = preprocessing(row)\n",
    "X_train, X_test, Y_train, Y_test = feature_extraction(row, train_set, None)\n",
    "Y_pred = estimation(row, ['shot_made_flag'], None, None, None)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if meta['name'].loc[row] == 'uciml_sms-spam-collection-dataset':\n",
    "    train_set = preprocessing(row)\n",
    "    X_train, X_test, Y_train, Y_test = feature_extraction(row, train_set, None)\n",
    "    Y_pred = estimation(row, X_train, X_test, Y_train, Y_test)\n",
    "    print(Y_pred)\n",
    "    \n",
    "if meta['name'].loc[row] == 'kinguistics_heartbeat-sounds':\n",
    "    X_train, X_test, Y_train, Y_test = preprocessing(row)\n",
    "    estimation(row,X_train,X_test,Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocessing(estimators,stack):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use 1.539588212966919 seconds to run this.\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Use\", end - start, \"seconds to run this.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
